{% extends "base.html" %}

{% block title %}AI Companion{% endblock %}

{% block content %}
<div class="max-w-4xl mx-auto">
    <!-- Chat Header -->
    <div class="bg-white rounded-t-lg shadow-sm p-4 border-b">
        <h1 class="text-2xl font-bold text-gray-900">Voice Chat</h1>
        <p class="text-gray-500 mt-1">Have a natural conversation with your AI companion</p>
    </div>

    <!-- Chat Messages -->
    <div class="bg-gray-50 h-[500px] overflow-y-auto p-4 space-y-4" id="chatMessages">
        <!-- Welcome Message -->
        <div class="flex items-start">
            <div class="flex-shrink-0">
                <div class="h-10 w-10 rounded-full bg-primary-500 flex items-center justify-center">
                    <svg class="h-6 w-6 text-white" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 10h.01M12 10h.01M16 10h.01M9 16H5a2 2 0 01-2-2V6a2 2 0 012-2h14a2 2 0 012 2v8a2 2 0 01-2 2h-5l-5 5v-5z" />
                    </svg>
                </div>
            </div>
            <div class="ml-3">
                <div class="bg-white rounded-lg shadow-sm p-4 max-w-lg">
                    <p class="text-gray-900">Hello! I'm your AI companion. Click the microphone button and start speaking to chat with me.</p>
                </div>
            </div>
        </div>
    </div>

    <!-- Chat Controls -->
    <div class="bg-white rounded-b-lg shadow-sm p-4 border-t">
        <div class="flex items-center justify-between">
            <div class="flex items-center space-x-4">
                <button id="micButton" 
                        class="flex items-center px-4 py-2 border border-transparent text-sm font-medium rounded-md shadow-sm text-white bg-primary-600 hover:bg-primary-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-primary-500 transition-all duration-200">
                    <svg class="h-5 w-5 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
                    </svg>
                    Start Recording
                </button>
                <span id="status" class="text-sm text-gray-500 italic"></span>
            </div>
            <div class="flex items-center">
                <div id="recordingIndicator" class="hidden">
                    <span class="flex h-3 w-3">
                        <span class="animate-ping absolute inline-flex h-3 w-3 rounded-full bg-red-400 opacity-75"></span>
                        <span class="relative inline-flex rounded-full h-3 w-3 bg-red-500"></span>
                    </span>
                </div>
            </div>
        </div>
    </div>
</div>
{% endblock %}

{% block scripts %}
<script>
let isRecording = false;
let mediaRecorder = null;
let audioContext = null;
let analyser = null;
let audioProcessor = null;
let silenceTimer = null;
let speechSynthesis = window.speechSynthesis;
let voices = [];
let recognition = null;

// Audio processing settings
const BUFFER_SIZE = 4096;
const SAMPLE_RATE = 44100;
const SILENCE_THRESHOLD = -65; // dB (more forgiving)
const SILENCE_DURATION = 3000; // 3 seconds (more time before stopping)

// Load voices
function loadVoices() {
    voices = speechSynthesis.getVoices();
}

if (speechSynthesis.onvoiceschanged !== undefined) {
    speechSynthesis.onvoiceschanged = loadVoices;
}

document.getElementById('micButton').addEventListener('click', toggleRecording);

function initializeSpeechRecognition() {
    if (!recognition) {
        recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        recognition.lang = 'en-US';
        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.maxAlternatives = 3;

        let finalTranscript = '';

        recognition.onstart = () => {
            console.log('Speech recognition started');
            document.getElementById('status').textContent = 'Listening... Click Stop when done speaking';
        };

        recognition.onresult = (event) => {
            let interimTranscript = '';
            
            for (let i = event.resultIndex; i < event.results.length; i++) {
                const transcript = event.results[i][0].transcript;
                if (event.results[i].isFinal) {
                    finalTranscript += transcript;
                    resetSilenceDetection();
                } else {
                    interimTranscript += transcript;
                }
            }

            if (interimTranscript) {
                document.getElementById('status').textContent = 'âœ“ Heard: ' + interimTranscript;
                document.getElementById('status').className = 'text-sm text-green-600 italic font-medium';
            }
        };

        recognition.onerror = (event) => {
            console.error('Speech recognition error:', event.error);
            const errorMessages = {
                'no-speech': 'No speech detected. Please speak louder or check your microphone.',
                'aborted': 'Speech recognition was interrupted. Please try again.',
                'audio-capture': 'No microphone detected. Please check your microphone connection.',
                'network': 'Network error occurred. Please check your internet connection.',
                'not-allowed': 'Microphone permission denied. Please allow microphone access.',
                'service-not-allowed': 'Speech recognition service not available in your browser.',
                'bad-grammar': 'Speech recognition configuration error.',
                'language-not-supported': 'Selected language is not supported.'
            };
            const errorMessage = errorMessages[event.error] || `Speech recognition error: ${event.error}`;
            document.getElementById('status').textContent = errorMessage;
            document.getElementById('status').className = 'text-sm text-red-600 italic font-medium';
            if (event.error === 'not-allowed') {
                showError('Please allow microphone access in your browser settings to use voice chat.');
            }
        };

        recognition.onend = () => {
            if (finalTranscript) {
                addMessage(finalTranscript, 'user');
                sendAudioAndMessageToAI(finalTranscript);
                finalTranscript = '';
            }
            console.log('Speech recognition ended');
        };
    }
}

function setupAudioProcessing(stream) {
    audioContext = new (window.AudioContext || window.webkitAudioContext)();
    analyser = audioContext.createAnalyser();
    const source = audioContext.createMediaStreamSource(stream);
    
    // Configure analyser
    analyser.fftSize = 2048;
    analyser.smoothingTimeConstant = 0.8;
    analyser.minDecibels = -90;
    analyser.maxDecibels = -10;
    
    // Create script processor for audio processing
    audioProcessor = audioContext.createScriptProcessor(BUFFER_SIZE, 1, 1);
    
    // Connect nodes
    source.connect(analyser);
    analyser.connect(audioProcessor);
    // Remove connection to destination to prevent audio playback
    
    // Process audio data
    const audioChunks = [];
    let isRecordingChunk = false;
    
    audioProcessor.onaudioprocess = (e) => {
        if (!isRecording) return;
        
        const inputData = e.inputBuffer.getChannelData(0);
        
        // Check audio level
        const dataArray = new Float32Array(analyser.frequencyBinCount);
        analyser.getFloatFrequencyData(dataArray);
        const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
        
        if (average < SILENCE_THRESHOLD) {
            if (!silenceTimer) {
                silenceTimer = setTimeout(() => {
                    if (isRecording) {
                        console.log('Silence detected, stopping recording');
                        toggleRecording();
                    }
                }, SILENCE_DURATION);
            }
            if (isRecordingChunk) {
                isRecordingChunk = false;
                const blob = new Blob(audioChunks, { type: 'audio/webm;codecs=opus' });
                uploadAudioChunk(blob);
                audioChunks.length = 0;
            }
        } else {
            resetSilenceDetection();
            if (!isRecordingChunk) {
                isRecordingChunk = true;
            }
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                audioChunks.push(new Float32Array(inputData));
            }
        }
    };
}

function resetSilenceDetection() {
    if (silenceTimer) {
        clearTimeout(silenceTimer);
        silenceTimer = null;
    }
}

async function uploadAudioChunk(blob) {
    const requestId = uuidv4();
    const formData = new FormData();
    formData.append('audio', blob);
    formData.append('request_id', requestId);
    
    try {
        const response = await fetch('/companion/upload-audio', {
            method: 'POST',
            body: formData
        });

        if (!response.ok) {
            throw new Error('Failed to upload audio');
        }

        const data = await response.json();
        console.log('Audio chunk uploaded:', data);
        
    } catch (error) {
        console.error('Error uploading audio chunk:', error);
        showError('Failed to upload audio chunk');
    }
}

function toggleRecording() {
    const button = document.getElementById('micButton');
    const status = document.getElementById('status');
    const recordingIndicator = document.getElementById('recordingIndicator');
    
    if (!isRecording) {
        initializeSpeechRecognition();
        
        navigator.mediaDevices.getUserMedia({ 
            audio: {
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true,
                sampleRate: SAMPLE_RATE
            }
        })
        .then(stream => {
            isRecording = true;
            button.classList.add('bg-red-600', 'hover:bg-red-700');
            button.classList.remove('bg-primary-600', 'hover:bg-primary-700', 'opacity-50', 'cursor-not-allowed');
            button.innerHTML = `
                <svg class="h-5 w-5 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
                </svg>
                Stop Recording
            `;
            status.textContent = 'Recording... Speak clearly into your microphone';
            status.className = 'text-sm text-green-600 italic font-medium';
            recordingIndicator.classList.remove('hidden');
            
            setupAudioProcessing(stream);
            recognition.start();
            
            mediaRecorder = new MediaRecorder(stream, {
                mimeType: 'audio/webm;codecs=opus'
            });
            mediaRecorder.start();
            
        })
        .catch(error => {
            console.error('Error accessing microphone:', error);
            status.textContent = 'Error: Could not access microphone';
            status.className = 'text-sm text-red-600 italic font-medium';
            showError('Please ensure your microphone is connected and you have granted permission to use it.');
        });
    } else {
        isRecording = false;
        button.classList.remove('bg-red-600', 'hover:bg-red-700');
        button.classList.add('bg-primary-600', 'hover:bg-primary-700');
        button.innerHTML = `
            <svg class="h-5 w-5 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
            </svg>
            Start Recording
        `;
        status.textContent = '';
        recordingIndicator.classList.add('hidden');
        
        if (recognition) {
            recognition.stop();
        }
        if (mediaRecorder && mediaRecorder.state !== 'inactive') {
            mediaRecorder.stop();
        }
        if (mediaRecorder && mediaRecorder.stream) {
            mediaRecorder.stream.getTracks().forEach(track => track.stop());
        }
        if (audioContext) {
            audioProcessor.disconnect();
            analyser.disconnect();
            audioContext.close();
            audioContext = null;
        }
        resetSilenceDetection();
    }
}

function uuidv4() {
    return ([1e7]+-1e3+-4e3+-8e3+-1e11).replace(/[018]/g, c =>
        (c ^ crypto.getRandomValues(new Uint8Array(1))[0] & 15 >> c / 4).toString(16)
    );
}

const pendingRequests = new Map();

async function sendMessageToAI(message) {
    const requestId = uuidv4();
    const timestamp = Date.now();
    
    pendingRequests.set(requestId, {
        message,
        timestamp
    });

    fetch('/companion/chat', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
        },
        body: JSON.stringify({ 
            message: message,
            request_id: requestId
        })
    })
    .then(response => {
        if (!response.ok) {
            throw new Error('Network response was not ok');
        }
        return response.json();
    })
    .then(data => {
        if (pendingRequests.has(data.request_id)) {
            const request = pendingRequests.get(data.request_id);
            const timeDiff = Date.now() - request.timestamp;
            if (timeDiff < 30000) {
                addMessage(data.message, 'ai');
                speakMessage(data.message);
            } else {
                console.log('Discarding old response:', data.request_id);
            }
            pendingRequests.delete(data.request_id);
        }
        document.getElementById('status').textContent = '';
    })
    .catch(error => {
        console.error('Error:', error);
        document.getElementById('status').textContent = '';
        showError('Error getting AI response. Please try again.');
        pendingRequests.delete(requestId);
    });

    const now = Date.now();
    for (const [id, request] of pendingRequests.entries()) {
        if (now - request.timestamp > 30000) {
            pendingRequests.delete(id);
        }
    }
}

function addMessage(message, type) {
    const chatMessages = document.getElementById('chatMessages');
    const messageDiv = document.createElement('div');
    messageDiv.className = 'flex items-start ' + (type === 'user' ? 'justify-end' : '');
    
    const content = `
        ${type === 'user' ? '' : `
        <div class="flex-shrink-0">
            <div class="h-10 w-10 rounded-full bg-primary-500 flex items-center justify-center">
                <svg class="h-6 w-6 text-white" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 10h.01M12 10h.01M16 10h.01M9 16H5a2 2 0 01-2-2V6a2 2 0 012-2h14a2 2 0 012 2v8a2 2 0 01-2 2h-5l-5 5v-5z" />
                </svg>
            </div>
        </div>
        `}
        <div class="${type === 'user' ? 'mr-3' : 'ml-3'}">
            <div class="${type === 'user' ? 'bg-primary-600 text-white' : 'bg-white'} rounded-lg shadow-sm p-4 max-w-lg">
                <p class="${type === 'user' ? 'text-white' : 'text-gray-900'}">${message}</p>
            </div>
        </div>
    `;
    
    messageDiv.innerHTML = content;
    chatMessages.appendChild(messageDiv);
    chatMessages.scrollTop = chatMessages.scrollHeight;
}

function speakMessage(message) {
    speechSynthesis.cancel();

    const utterance = new SpeechSynthesisUtterance(message);
    
    const englishVoices = voices.filter(voice => voice.lang.startsWith('en-'));
    if (englishVoices.length > 0) {
        const femaleVoice = englishVoices.find(voice => voice.name.includes('female') || voice.name.includes('Female'));
        utterance.voice = femaleVoice || englishVoices[0];
    }
    
    utterance.rate = 1.0;
    utterance.pitch = 1.0;
    utterance.volume = 1.0;
    
    speechSynthesis.speak(utterance);
}

function showError(message) {
    const errorDiv = document.createElement('div');
    errorDiv.className = 'flex items-start mb-4';
    errorDiv.innerHTML = `
        <div class="flex-shrink-0">
            <div class="h-10 w-10 rounded-full bg-red-500 flex items-center justify-center">
                <svg class="h-6 w-6 text-white" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4m0 4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                </svg>
            </div>
        </div>
        <div class="ml-3">
            <div class="bg-red-50 text-red-700 rounded-lg shadow-sm p-4 max-w-lg">
                <p>${message}</p>
            </div>
        </div>
    `;
    
    const chatMessages = document.getElementById('chatMessages');
    chatMessages.appendChild(errorDiv);
    chatMessages.scrollTop = chatMessages.scrollHeight;
}

loadVoices();
</script>
{% endblock %}
