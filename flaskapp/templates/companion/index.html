{% extends "base.html" %}

{% block title %}AI Companion{% endblock %}

{% block content %}
<div class="max-w-4xl mx-auto">
    <!-- Chat Header -->
    <div class="bg-white rounded-t-lg shadow-sm p-4 border-b">
        <h1 class="text-2xl font-bold text-gray-900">Voice Chat</h1>
        <p class="text-gray-500 mt-1">Have a natural conversation with your AI companion</p>
    </div>

    <!-- Chat Messages -->
    <div class="bg-gray-50 h-[500px] overflow-y-auto p-4 space-y-4" id="chatMessages">
        <!-- Welcome Message -->
        <div class="flex items-start">
            <div class="flex-shrink-0">
                <div class="h-10 w-10 rounded-full bg-primary-500 flex items-center justify-center">
                    <svg class="h-6 w-6 text-white" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 10h.01M12 10h.01M16 10h.01M9 16H5a2 2 0 01-2-2V6a2 2 0 012-2h14a2 2 0 012 2v8a2 2 0 01-2 2h-5l-5 5v-5z" />
                    </svg>
                </div>
            </div>
            <div class="ml-3">
                <div class="bg-white rounded-lg shadow-sm p-4 max-w-lg">
                    <p class="text-gray-900">Hello! I'm your AI companion. Click the microphone button and start speaking to chat with me.</p>
                </div>
            </div>
        </div>
    </div>

    <!-- Chat Controls -->
    <div class="bg-white rounded-b-lg shadow-sm p-4 border-t">
        <div class="flex items-center justify-between">
            <div class="flex items-center space-x-4">
                <button id="micButton" 
                        class="flex items-center px-4 py-2 border border-transparent text-sm font-medium rounded-md shadow-sm text-white bg-primary-600 hover:bg-primary-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-primary-500 transition-all duration-200">
                    <svg class="h-5 w-5 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
                    </svg>
                    Start Recording
                </button>
                <span id="status" class="text-sm text-gray-500 italic"></span>
            </div>
            <div class="flex items-center">
                <div id="recordingIndicator" class="hidden">
                    <span class="flex h-3 w-3">
                        <span class="animate-ping absolute inline-flex h-3 w-3 rounded-full bg-red-400 opacity-75"></span>
                        <span class="relative inline-flex rounded-full h-3 w-3 bg-red-500"></span>
                    </span>
                </div>
            </div>
        </div>
    </div>
</div>
{% endblock %}

{% block scripts %}
<script>
let isRecording = false;
let mediaRecorder = null;
let audioChunks = [];
let speechSynthesis = window.speechSynthesis;
let voices = [];
let recognition = null;
let audioContext = null;
let analyser = null;
let silenceTimer = null;
const silenceThreshold = -50; // dB
const silenceTimeout = 2000; // 2 seconds of silence to auto-stop

// Load voices
function loadVoices() {
    voices = speechSynthesis.getVoices();
}

if (speechSynthesis.onvoiceschanged !== undefined) {
    speechSynthesis.onvoiceschanged = loadVoices;
}

document.getElementById('micButton').addEventListener('click', toggleRecording);

function initializeSpeechRecognition() {
    if (!recognition) {
        recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        recognition.lang = 'en-US';
        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.maxAlternatives = 3;

        let finalTranscript = '';

        recognition.onstart = () => {
            console.log('Speech recognition started');
            document.getElementById('status').textContent = 'Listening...';
        };

        recognition.onresult = (event) => {
            let interimTranscript = '';
            
            for (let i = event.resultIndex; i < event.results.length; i++) {
                const transcript = event.results[i][0].transcript;
                if (event.results[i].isFinal) {
                    finalTranscript += transcript;
                    // Reset silence detection on speech
                    resetSilenceDetection();
                } else {
                    interimTranscript += transcript;
                }
            }

            // Update status with interim results
            if (interimTranscript) {
                document.getElementById('status').textContent = 'Heard: ' + interimTranscript;
            }
        };

        recognition.onerror = (event) => {
            console.error('Speech recognition error:', event.error);
            if (event.error === 'no-speech') {
                document.getElementById('status').textContent = 'No speech detected. Please try again.';
            } else {
                document.getElementById('status').textContent = '';
                showError(`Speech recognition error: ${event.error}`);
            }
        };

        recognition.onend = () => {
            if (finalTranscript) {
                addMessage(finalTranscript, 'user');
                sendMessageToAI(finalTranscript);
                finalTranscript = '';
            }
            console.log('Speech recognition ended');
        };
    }
}

function setupAudioAnalysis(stream) {
    audioContext = new (window.AudioContext || window.webkitAudioContext)();
    analyser = audioContext.createAnalyser();
    const source = audioContext.createMediaStreamSource(stream);
    source.connect(analyser);
    analyser.fftSize = 2048;
    
    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Float32Array(bufferLength);
    
    function checkAudioLevel() {
        if (!isRecording) return;
        
        analyser.getFloatFrequencyData(dataArray);
        const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
        
        if (average < silenceThreshold) {
            if (!silenceTimer) {
                silenceTimer = setTimeout(() => {
                    if (isRecording) {
                        console.log('Silence detected, stopping recording');
                        toggleRecording();
                    }
                }, silenceTimeout);
            }
        } else {
            resetSilenceDetection();
        }
        
        requestAnimationFrame(checkAudioLevel);
    }
    
    checkAudioLevel();
}

function resetSilenceDetection() {
    if (silenceTimer) {
        clearTimeout(silenceTimer);
        silenceTimer = null;
    }
}

function toggleRecording() {
    const button = document.getElementById('micButton');
    const status = document.getElementById('status');
    const recordingIndicator = document.getElementById('recordingIndicator');
    
    if (!isRecording) {
        // Initialize speech recognition
        initializeSpeechRecognition();
        
        // Start recording
        navigator.mediaDevices.getUserMedia({ 
            audio: {
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true
            }
        })
        .then(stream => {
            isRecording = true;
            button.classList.add('bg-red-600', 'hover:bg-red-700');
            button.classList.remove('bg-primary-600', 'hover:bg-primary-700');
            button.innerHTML = `
                <svg class="h-5 w-5 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
                </svg>
                Stop Recording
            `;
            status.textContent = 'Recording...';
            recordingIndicator.classList.remove('hidden');
            
            // Setup audio analysis for silence detection
            setupAudioAnalysis(stream);
            
            // Start speech recognition
            recognition.start();
            
            // Setup media recorder
            mediaRecorder = new MediaRecorder(stream, {
                mimeType: 'audio/webm;codecs=opus'
            });
            audioChunks = [];
            
            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    audioChunks.push(event.data);
                }
            };
            
            mediaRecorder.start(100); // Collect data every 100ms
        })
        .catch(error => {
            console.error('Error accessing microphone:', error);
            status.textContent = 'Error accessing microphone';
            showError('Please ensure your microphone is connected and you have granted permission to use it.');
        });
    } else {
        // Stop recording
        isRecording = false;
        button.classList.remove('bg-red-600', 'hover:bg-red-700');
        button.classList.add('bg-primary-600', 'hover:bg-primary-700');
        button.innerHTML = `
            <svg class="h-5 w-5 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
            </svg>
            Start Recording
        `;
        status.textContent = '';
        recordingIndicator.classList.add('hidden');
        
        // Stop all recording components
        if (recognition) {
            recognition.stop();
        }
        if (mediaRecorder && mediaRecorder.state !== 'inactive') {
            mediaRecorder.stop();
        }
        if (mediaRecorder && mediaRecorder.stream) {
            mediaRecorder.stream.getTracks().forEach(track => track.stop());
        }
        if (audioContext) {
            audioContext.close();
            audioContext = null;
        }
        resetSilenceDetection();
    }
}

// Generate UUID v4
function uuidv4() {
    return ([1e7]+-1e3+-4e3+-8e3+-1e11).replace(/[018]/g, c =>
        (c ^ crypto.getRandomValues(new Uint8Array(1))[0] & 15 >> c / 4).toString(16)
    );
}

// Keep track of pending requests
const pendingRequests = new Map();

function sendMessageToAI(message) {
    const requestId = uuidv4();
    const timestamp = Date.now();
    
    pendingRequests.set(requestId, {
        message,
        timestamp
    });

    fetch('/companion/chat', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
        },
        body: JSON.stringify({ 
            message: message,
            request_id: requestId
        })
    })
    .then(response => {
        if (!response.ok) {
            throw new Error('Network response was not ok');
        }
        return response.json();
    })
    .then(data => {
        // Check if this response matches a pending request
        if (pendingRequests.has(data.request_id)) {
            const request = pendingRequests.get(data.request_id);
            // Only process if this is the most recent request or if the time difference is small
            const timeDiff = Date.now() - request.timestamp;
            if (timeDiff < 30000) { // 30 seconds threshold
                addMessage(data.message, 'ai');
                speakMessage(data.message);
            } else {
                console.log('Discarding old response:', data.request_id);
            }
            // Clean up the pending request
            pendingRequests.delete(data.request_id);
        }
        document.getElementById('status').textContent = '';
    })
    .catch(error => {
        console.error('Error:', error);
        document.getElementById('status').textContent = '';
        showError('Error getting AI response. Please try again.');
        // Clean up the pending request on error
        pendingRequests.delete(requestId);
    });

    // Clean up old pending requests
    const now = Date.now();
    for (const [id, request] of pendingRequests.entries()) {
        if (now - request.timestamp > 30000) { // Remove requests older than 30 seconds
            pendingRequests.delete(id);
        }
    }
}

function addMessage(message, type) {
    const chatMessages = document.getElementById('chatMessages');
    const messageDiv = document.createElement('div');
    messageDiv.className = 'flex items-start ' + (type === 'user' ? 'justify-end' : '');
    
    const content = `
        ${type === 'user' ? '' : `
        <div class="flex-shrink-0">
            <div class="h-10 w-10 rounded-full bg-primary-500 flex items-center justify-center">
                <svg class="h-6 w-6 text-white" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 10h.01M12 10h.01M16 10h.01M9 16H5a2 2 0 01-2-2V6a2 2 0 012-2h14a2 2 0 012 2v8a2 2 0 01-2 2h-5l-5 5v-5z" />
                </svg>
            </div>
        </div>
        `}
        <div class="${type === 'user' ? 'mr-3' : 'ml-3'}">
            <div class="${type === 'user' ? 'bg-primary-600 text-white' : 'bg-white'} rounded-lg shadow-sm p-4 max-w-lg">
                <p class="${type === 'user' ? 'text-white' : 'text-gray-900'}">${message}</p>
            </div>
        </div>
    `;
    
    messageDiv.innerHTML = content;
    chatMessages.appendChild(messageDiv);
    chatMessages.scrollTop = chatMessages.scrollHeight;
}

function speakMessage(message) {
    // Cancel any ongoing speech
    speechSynthesis.cancel();

    const utterance = new SpeechSynthesisUtterance(message);
    
    // Try to find a good English voice
    const englishVoices = voices.filter(voice => voice.lang.startsWith('en-'));
    if (englishVoices.length > 0) {
        // Prefer female voices if available
        const femaleVoice = englishVoices.find(voice => voice.name.includes('female') || voice.name.includes('Female'));
        utterance.voice = femaleVoice || englishVoices[0];
    }
    
    utterance.rate = 1.0;
    utterance.pitch = 1.0;
    utterance.volume = 1.0;
    
    speechSynthesis.speak(utterance);
}

function showError(message) {
    const errorDiv = document.createElement('div');
    errorDiv.className = 'flex items-start mb-4';
    errorDiv.innerHTML = `
        <div class="flex-shrink-0">
            <div class="h-10 w-10 rounded-full bg-red-500 flex items-center justify-center">
                <svg class="h-6 w-6 text-white" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4m0 4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                </svg>
            </div>
        </div>
        <div class="ml-3">
            <div class="bg-red-50 text-red-700 rounded-lg shadow-sm p-4 max-w-lg">
                <p>${message}</p>
            </div>
        </div>
    `;
    
    const chatMessages = document.getElementById('chatMessages');
    chatMessages.appendChild(errorDiv);
    chatMessages.scrollTop = chatMessages.scrollHeight;
}

// Load voices when the page loads
loadVoices();
</script>
{% endblock %}
